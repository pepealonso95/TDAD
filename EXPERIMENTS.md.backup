# TDAD Thesis: Experiment Log

**Project**: Test-Driven AI Development - Minimizing Code Regressions in AI Agents
**Author**: Rafael Alonso
**Repository**: https://github.com/pepealonso95/TDAD

---

## Experiment Template

Each experiment entry should include:
- **Experiment ID**: Unique identifier (e.g., EXP-001)
- **Date**: When the experiment was conducted
- **Configuration**: What was changed from baseline
- **Hypothesis**: What we expect to happen
- **Method**: How the experiment was run
- **Results**: Generation rate, evaluation score, regression rate
- **Analysis**: What we learned
- **Next Steps**: What to do next

---

## EXP-001: Baseline Evaluation

### Metadata
- **Date**: October 27-28, 2025
- **Configuration**: Vanilla Claude Code with default SWE-bench prompts
- **Model**: Claude Sonnet 4.5 (default)
- **Dataset**: SWE-bench Lite
- **Sample Size**: 10 instances (quick test)

### Hypothesis
Establish baseline regression rate for Claude Code without modifications. Expected performance based on Anthropic's published results: 15-25% resolution rate.

### Method
```bash
python code_swe_agent.py \
  --dataset_name princeton-nlp/SWE-bench_Lite \
  --limit 10 \
  --backend claude
```

**Configuration Files**:
- Prompt: `prompts/swe_bench_prompt.txt` (default)
- Model Registry: Fixed to use Claude Code aliases
- No additional plugins or modifications

### Results

#### Generation Phase
- **Duration**: 49 minutes 18 seconds
- **Generation Rate**: 80% (8/10 patches generated)
- **Average Time per Instance**: ~5 minutes
- **Empty Patches**: 2/10 (20%)

#### Successful Patches Generated
1. ‚úÖ **astropy__astropy-12907** (3:59)
   - Issue: Separability matrix for nested CompoundModels
   - Patch Size: 17,292 characters
   - Changes: Fixed `_cstack` function bug + comprehensive tests

2. ‚úÖ **astropy__astropy-14182** (5:21)
   - Issue: RST header_rows support
   - Patch Size: 4,092 characters
   - Changes: Modified RST class to accept header_rows parameter

3. ‚úÖ **astropy__astropy-14365** (3:13)
   - Issue: QDP case-sensitive commands
   - Patch Size: 3,154 characters
   - Changes: Added re.IGNORECASE flag to regex

4. ‚úÖ **astropy__astropy-14995** (4:53)
   - Issue: NDDataRef mask propagation
   - Patch Size: 9,728 characters
   - Changes: Fixed mask handling when operand has no mask

5. ‚úÖ **astropy__astropy-6938** (4:19)
   - Issue: FITS D exponents bug
   - Patch Size: 5,186 characters
   - Changes: Test files only (no actual code changes detected)

6. ‚úÖ **astropy__astropy-7746** (5:20)
   - Issue: WCS empty arrays handling
   - Patch Size: 7,034 characters
   - Changes: Added empty array checks in WCS transformations

7. ‚úÖ **django__django-10914** (5:20)
   - Issue: FILE_UPLOAD_PERMISSION default
   - Patch Size: 7,782 characters
   - Changes: Set default to 0o644, updated docs and tests

8. ‚úÖ **django__django-10924** (4:14)
   - Issue: FilePathField callable support
   - Patch Size: 3,245 characters
   - Changes: Allow callable paths to fix migration issues

#### Failed Cases
9. ‚ùå **django__django-11001** (5:15)
   - Issue: Multiline RawSQL order_by removal
   - Error: Git repository corruption ("Not a git repository")
   - Root Cause: Directory state lost during execution

10. ‚ùå (Instance not logged - likely similar directory issue)

#### Evaluation Phase (Pending)
- **Status**: Not yet run on full 10 instances
- **Single Instance Test**: 1/1 passed (100% for astropy__astropy-12907)
- **Expected Full Evaluation Time**: ~20 minutes

### Analysis

#### Key Findings

1. **High Generation Rate (80%)**
   - Claude Code successfully attempts fixes on most instances
   - Better than expected for vanilla configuration
   - Only 20% empty patches (agent confusion/analysis-only cases)

2. **Patch Quality Observations**
   - Patches include actual code fixes + tests + documentation
   - Average patch size: 6,939 characters (substantial changes)
   - Agent follows issue descriptions well
   - Implements fixes with proper error handling

3. **Common Failure Patterns**
   - Git repository corruption (directory management issues)
   - Claude Code sometimes cleans up working directory
   - FileNotFoundError when restoring cwd after execution

4. **Performance Characteristics**
   - Execution time highly variable: 3-5 minutes per instance
   - No correlation between patch size and execution time
   - Dataset loading takes ~2-3 minutes initially

#### Toolkit Issues Fixed During Experiment

1. **Model Registry** (`utils/model_registry.py`)
   - Original: Used full model IDs (e.g., `claude-sonnet-4-20250815`)
   - Problem: 404 errors from API
   - Fix: Map to Claude Code aliases (`sonnet`, `opus`)
   - Impact: Enabled successful execution

2. **Debug Logging** (`utils/claude_interface.py`, `utils/patch_extractor.py`)
   - Added: Command preview, stdout/stderr capture, git status
   - Purpose: Visibility into agent behavior for debugging
   - Result: Identified directory management issues

3. **Error Handling** (`utils/claude_interface.py`)
   - Added: Try-catch around `os.chdir()` restore
   - Purpose: Graceful handling of directory cleanup
   - Result: Prevented cascading failures

### Limitations & Concerns

1. **Directory Management**
   - Claude Code's workspace cleanup interferes with toolkit
   - May cause underestimation of success rate
   - Need to investigate why some repos lose git state

2. **Evaluation Pending**
   - Generation rate ‚â† resolution rate
   - Don't know yet if patches actually fix issues
   - Docker evaluation required to get real scores

3. **Sample Size**
   - Only 10 instances for quick test
   - May not be representative of full benchmark
   - Need larger sample for statistical significance

4. **Git State Corruption**
   - 2/10 instances lost git repository
   - Possibly related to Claude Code's directory operations
   - May need isolation improvements

### Next Steps

1. **Immediate**
   - [ ] Run Docker evaluation on 8 successful patches
   - [ ] Calculate actual resolution rate (THE REAL SCORE)
   - [ ] Analyze which types of issues Claude handles well

2. **Baseline Completion**
   - [ ] Run full 300-instance benchmark (or 50-instance sample)
   - [ ] Collect statistically significant baseline data
   - [ ] Identify regression patterns in failures

3. **Preparation for EXP-002**
   - [ ] Design TDD prompt modifications
   - [ ] Create prompt template enforcing test-first workflow
   - [ ] Plan A/B comparison methodology

### Raw Data

**Predictions File**: `predictions/predictions_20251027_205019.jsonl`
**Benchmark Log**: `benchmark_scores.log`
**Repository State**: Commit `711b164`

### Expected vs Actual

| Metric | Expected (Anthropic) | Actual |
|--------|---------------------|---------|
| Generation Rate | N/A | 80% |
| Resolution Rate | 15-25% | TBD (pending eval) |
| Execution Time | ~5 min/instance | 4.9 min/instance |

### Conclusion

Baseline successfully established. The toolkit works with Claude Code after fixes. Generation rate of 80% is promising, but true test is the evaluation phase to determine how many patches actually resolve issues without introducing regressions.

The agent demonstrates sophisticated behavior (tests + docs + fixes) but has edge cases around directory management that need addressing for production use.

**Status**: ‚úÖ Generation Complete, ‚è≥ Evaluation Pending

---

## EXP-001B: Model Comparison - Haiku vs Sonnet

### Metadata
- **Date**: October 28, 2025 (23:00-23:10)
- **Configuration**: Controlled comparison on identical instance
- **Models**: Claude Haiku 4.5, Claude Sonnet 4.5
- **Dataset**: SWE-bench Lite (single instance)
- **Sample Size**: 1 instance √ó 2 runs (1 Haiku, 1 Sonnet)
- **Purpose**: Compare runtime and patch quality between models

### Hypothesis
Based on earlier runs and general model characteristics:
1. Haiku should be faster (smaller model = faster inference)
2. Haiku should produce more minimal patches (less comprehensive)
3. Sonnet should produce more comprehensive patches (tests + docs)
4. Both models should produce the correct core fix

### Method

**Commands Run**:
```bash
# Haiku 4.5 test (logged to /tmp/haiku_test.log)
python code_swe_agent.py --dataset_name princeton-nlp/SWE-bench_Lite --limit 1 --backend claude --model haiku

# Sonnet 4.5 test (logged to /tmp/sonnet_test.log)
python code_swe_agent.py --dataset_name princeton-nlp/SWE-bench_Lite --limit 1 --backend claude
```

**Test Configuration**:
- Same instance tested: `astropy__astropy-12907` (Separability matrix bug)
- Clean repository clones for each test
- No caching between runs
- Full debug logging enabled

### Results

**Instance Tested**: `astropy__astropy-12907` (Separability matrix for nested CompoundModels)

| Metric | Haiku 4.5 | Sonnet 4.5 | Difference |
|--------|-----------|------------|------------|
| **Total Runtime** | 2:35 (155s) | 4:09 (250s) | +95s (61% slower) |
| **Patch Size** | 506 chars | 12,644 chars | 25x larger |
| **Core Fix** | ‚úÖ Correct | ‚úÖ Correct | Identical |
| **Files Modified** | 1 file | 2 files + extras | More comprehensive |
| **Test Coverage** | None | Extensive | Significant difference |

**Core Fix (Identical)**:
```python
# File: astropy/modeling/separable.py, Line 245
# Before (bug)
cright[-right.shape[0]:, -right.shape[1]:] = 1

# After (fix)
cright[-right.shape[0]:, -right.shape[1]:] = right
```

**Detailed Breakdown**:

**Haiku 4.5 Results**:
- **Runtime**: 2:35 (155 seconds)
- **Modified**: `astropy/modeling/separable.py` only
- **Patch Size**: 506 characters
- **Approach**: Minimal fix - changed only the buggy line
- **Git Status**: 1 modified file
- **Prediction File**: `predictions_20251028_225825.jsonl`

**Sonnet 4.5 Results**:
- **Runtime**: 4:09 (250 seconds)
- **Modified**: `astropy/modeling/separable.py`, `astropy/modeling/tests/test_separable.py`
- **Added**: `docs/changes/modeling/12907.bugfix.rst`, `test_issue.py`, `test_nested_compound_fix.py`, `verify_fix.py`
- **Patch Size**: 12,644 characters
- **Approach**: Comprehensive fix with extensive validation
- **Git Status**: 2 modified + 4 untracked files
- **Prediction File**: `predictions_20251028_230313.jsonl`

### Analysis

#### Key Findings

1. **Speed Difference: 61% Slower for Sonnet**
   - Haiku: 2:35 (155s)
   - Sonnet: 4:09 (250s)
   - Additional 95 seconds for Sonnet
   - Likely due to both:
     - Slower model inference (larger model)
     - More comprehensive work (tests + docs + validation)

2. **Patch Quality: Minimal vs Comprehensive**
   - Haiku: 506 chars (1 file, core fix only)
   - Sonnet: 12,644 chars (6 files, fix + tests + docs + validation)
   - **25x size difference**
   - Both produce identical core fix
   - Sonnet adds significant validation infrastructure

3. **Correctness: Both Successful** ‚úÖ
   - Both models identified the exact same bug
   - Both applied the identical fix
   - Both return code 0 (successful execution)
   - Both generated valid patches

4. **Cost-Benefit Analysis**
   - Haiku: Faster (39% of Sonnet's time) + likely 10-20x cheaper
   - Sonnet: More comprehensive (tests reduce regression risk)
   - For rapid iteration: Haiku wins
   - For production quality: Sonnet wins

5. **Previous vs Current Comparison**
   - **Previous tests** (EXP-001): Haiku 4:43, Sonnet 4:59 (nearly identical)
   - **Current tests** (EXP-001B): Haiku 2:35, Sonnet 4:09 (61% difference)
   - **Why?**: Previous Haiku run included comprehensive tests; current run is minimal
   - **Conclusion**: Haiku's variability in comprehensiveness affects runtime

#### Unexpected Observations

1. **Haiku's Minimal Approach**:
   - This run produced only the core fix (no tests)
   - Previous runs (EXP-001) showed Haiku adding tests
   - Suggests non-deterministic behavior in comprehensiveness
   - Temperature/sampling likely affects "how much to do"

2. **Sonnet's Consistency**:
   - Always produces comprehensive patches
   - Adds tests, docs, validation scripts
   - More predictable output
   - Higher quality baseline

3. **Runtime Variability**:
   - Haiku: 2:35 to 4:45 depending on comprehensiveness
   - Sonnet: 4:09 to 4:59 (more consistent)
   - Most variance from "how much work" not "model speed"

#### Implications for Thesis

1. **Model Selection Strategy**:
   - **Haiku**: Cost-effective for initial passes, rapid iteration
     - 61% faster (when minimal)
     - 10-20x cheaper
     - Correct core fixes
     - Risk: May skip important validation

   - **Sonnet**: Production use, comprehensive validation
     - Consistent comprehensiveness
     - Extensive test coverage
     - Better regression prevention
     - Cost: Slower + more expensive

2. **Evaluation Priority**:
   - Need to run Docker evaluation to see if Sonnet's extra tests actually help
   - Key question: Does comprehensive patch improve resolution rate?
   - Or does minimal fix work just as well?

3. **Experiment Design**:
   - Use Haiku for large-scale testing (300 instances)
   - Use Sonnet for production-quality experiments
   - Consider hybrid: Haiku first pass, Sonnet for refinement

### Toolkit Validation

All toolkit components working correctly during this test:

‚úÖ **Model Registry**: Correct mapping for both Haiku and Sonnet
‚úÖ **Error Handling**: No directory management failures
‚úÖ **Debug Logging**: Full output captured to log files
‚úÖ **Haiku Support**: Executes cleanly with minimal patch generation
‚úÖ **Sonnet Support**: Executes cleanly with comprehensive patch generation

### Next Steps

- [x] Compare Haiku vs Sonnet runtime and quality
- [x] Validate model registry and toolkit fixes
- [x] Document findings in EXPERIMENTS.md
- [ ] Run Docker evaluation on both patches to compare resolution rates
- [ ] Determine if comprehensive tests improve actual pass rates
- [ ] Decide model selection for future experiments:
  - EXP-002 (TDD prompts)
  - Full 300-instance baseline run
- [ ] Consider cost-benefit tradeoff (Haiku savings vs Sonnet quality)

### Raw Data

**Log Files**:
- Haiku: `/tmp/haiku_test.log`
- Sonnet: `/tmp/sonnet_test.log`

**Prediction Files**:
- Haiku: `predictions/predictions_20251028_225825.jsonl` (506 chars)
- Sonnet: `predictions/predictions_20251028_230313.jsonl` (12,644 chars)

**Repository State**: Commit TBD (after updating EXPERIMENTS.md)

### Conclusion

**Haiku 4.5 is significantly faster (61%) but produces minimal patches.** Both models correctly identify and fix the core bug, but Sonnet adds comprehensive test coverage that may prevent regressions.

**Key Tradeoff**:
- Haiku: 2:35 runtime, 506 char patch, 10-20x cheaper ‚Üí Best for rapid iteration
- Sonnet: 4:09 runtime, 12,644 char patch, more expensive ‚Üí Best for comprehensive validation

**Critical Question**: Does Sonnet's additional test coverage actually improve SWE-bench resolution rates? This requires Docker evaluation to determine if comprehensive patches pass more tests than minimal patches.

**Recommendation**:
- Use Haiku for large-scale testing (300 instances) to save time and cost
- Use Sonnet for production experiments (TDD, GraphRAG) where quality matters most
- Consider hybrid approach: Haiku generates initial patches, Sonnet refines failures

**Status**: ‚úÖ Complete - Model Comparison Documented

---

## EXP-001C: SWE-bench Verified Dataset Test

### Metadata
- **Date**: October 28, 2025 (23:16)
- **Configuration**: Switch from SWE-bench Lite to SWE-bench Verified
- **Model**: Claude Haiku 4.5
- **Dataset**: SWE-bench Verified (500 human-validated instances)
- **Sample Size**: 1 instance
- **Purpose**: Validate toolkit works with SWE-bench Verified dataset

### Hypothesis
The toolkit should work seamlessly with SWE-bench Verified, which contains 500 human-validated test instances (vs 300 in Lite). This dataset has higher quality assurance and is better for production evaluation.

### Method

**Dataset Change**:
- **Previous**: `princeton-nlp/SWE-bench_Lite` (300 instances)
- **New**: `princeton-nlp/SWE-bench_Verified` (500 instances, human-validated)

**Command**:
```bash
python code_swe_agent.py \
  --dataset_name princeton-nlp/SWE-bench_Verified \
  --limit 1 \
  --backend claude \
  --model haiku
```

### Results

**Instance Tested**: `astropy__astropy-12907` (same as EXP-001B tests)

| Metric | Result |
|--------|--------|
| **Total Runtime** | 2:04 (124 seconds) |
| **Patch Size** | 2,037 characters |
| **Core Fix** | ‚úÖ Correct (line 245) |
| **Files Modified** | 2 (separable.py + test_separable.py) |
| **Test Coverage** | Comprehensive regression test added |
| **Generation Status** | ‚úÖ Success |

**Core Fix (Identical to EXP-001B)**:
```python
# File: astropy/modeling/separable.py, Line 245
# Before (bug)
cright[-right.shape[0]:, -right.shape[1]:] = 1

# After (fix)
cright[-right.shape[0]:, -right.shape[1]:] = right
```

**Test Coverage Added**:
- New test function: `test_nested_compound_models_separability()`
- 36 lines of test code
- Tests both nested and non-nested compound models
- Includes expected separability matrix validation
- Documents regression test for issue #12907

### Analysis

#### Key Findings

1. **Dataset Compatibility** ‚úÖ
   - SWE-bench Verified loaded successfully (500 instances)
   - No code changes required in toolkit
   - Same interface as SWE-bench Lite
   - Fast loading (dataset cached locally)

2. **Haiku Performance Consistency**
   - Runtime: 2:04 (124s) vs previous 2:35 (155s)
   - **20% faster** than EXP-001B Haiku test
   - Produced comprehensive patch (2,037 chars) vs minimal (506 chars)
   - Shows Haiku's non-deterministic comprehensiveness again

3. **Patch Quality: Comprehensive**
   - This time Haiku added test coverage (unlike EXP-001B minimal approach)
   - 2,037 characters: between EXP-001B Haiku (506) and Sonnet (12,644)
   - Regression test included with proper documentation
   - Professional code quality

4. **Same Instance, Different Behavior**
   - Same instance (`astropy__astropy-12907`) across EXP-001B and EXP-001C
   - EXP-001B Haiku: 506 chars, no tests
   - EXP-001C Haiku: 2,037 chars, comprehensive test
   - **4x size difference** on identical issue
   - Confirms Haiku's variability in approach

#### Haiku Behavior Patterns

**Observed Across Multiple Runs**:
| Run | Dataset | Runtime | Patch Size | Test Coverage |
|-----|---------|---------|------------|---------------|
| EXP-001B Run 1 | Lite | 2:35 (155s) | 506 chars | None |
| EXP-001C | Verified | 2:04 (124s) | 2,037 chars | Comprehensive |

**Pattern**: Haiku varies between:
- **Minimal mode**: Core fix only (506 chars)
- **Comprehensive mode**: Fix + tests + docs (2,037 chars)
- **Not correlated with dataset** (same instance, different behavior)
- Likely influenced by sampling/temperature

### Implications for Thesis

1. **Dataset Selection**: SWE-bench Verified is recommended for production experiments
   - 500 vs 300 instances (67% more data)
   - Human-validated quality
   - Same API, no code changes needed
   - Better statistical significance

2. **Haiku Variability Challenge**:
   - Cannot rely on consistent comprehensiveness
   - May need multiple runs per instance
   - Or use Sonnet for consistent quality
   - Consider temperature parameter tuning

3. **Future Experiment Design**:
   - **EXP-002 (TDD)**: Use SWE-bench Verified
   - **EXP-003 (Vector RAG)**: Use SWE-bench Verified
   - **EXP-004 (GraphRAG)**: Use SWE-bench Verified
   - Larger sample size for baseline (50-100 instances)

### Next Steps

- [x] Validate SWE-bench Verified dataset compatibility
- [x] Test Haiku on Verified dataset
- [x] Document findings in EXPERIMENTS.md
- [ ] Run larger baseline on SWE-bench Verified (10-50 instances)
- [ ] Compare Verified vs Lite instances (overlap analysis)
- [ ] Decide on final dataset for all experiments
- [ ] Document dataset selection rationale in thesis

### Raw Data

**Log File**: `/tmp/haiku_verified_test.log`
**Prediction File**: `predictions/predictions_20251028_231629.jsonl` (2,037 chars)
**Dataset**: `princeton-nlp/SWE-bench_Verified` (500 instances)

### Conclusion

**SWE-bench Verified is fully compatible and ready for production use.** The dataset loads quickly, has more instances (500 vs 300), and provides human-validated quality assurance.

Haiku's performance on Verified (2:04, 2,037 chars) demonstrates its capability to produce quality patches with test coverage, contrasting with the minimal approach observed in EXP-001B. This variability reinforces the need for either:
- Multiple runs per instance to capture best result
- Sonnet for consistent comprehensive quality
- Ensemble approach (Haiku + Sonnet)

**Recommendation**: Switch all future experiments to SWE-bench Verified for better quality and larger sample size.

**Status**: ‚úÖ Complete - SWE-bench Verified Validated

---

## EXP-002: TDD Prompt Engineering (Planned)

### Metadata
- **Date**: TBD
- **Configuration**: Modified prompt enforcing TDD workflow
- **Model**: Claude Sonnet 4.5 (default)
- **Dataset**: SWE-bench Lite
- **Sample Size**: 10 instances (initial), then 50 or 300

### Hypothesis
Enforcing TDD practices (write tests first, then implementation) will:
1. Reduce regression rate by catching breaking changes early
2. Potentially lower generation rate (more constraint = more failures)
3. Increase patch quality (test coverage guarantees)

### Method
**Prompt Modifications** (`prompts/swe_bench_tdd.txt`):
1. Require test creation before implementation
2. Mandate test execution and confirmation
3. Only allow code changes after tests pass
4. Instruct to run existing tests to catch regressions

**Command**:
```bash
python code_swe_agent.py \
  --dataset_name princeton-nlp/SWE-bench_Lite \
  --limit 10 \
  --backend claude \
  --prompt-template prompts/swe_bench_tdd.txt
```

### Expected Results
- Generation Rate: 60-70% (lower due to stricter requirements)
- Resolution Rate: 20-30% (higher than baseline)
- Regression Rate: <10% (main goal)

### Status
üî¥ **Not Started**

---

## EXP-003: Vector RAG with claude-context (Planned)

### Metadata
- **Date**: TBD
- **Configuration**: Baseline + claude-context plugin
- **Model**: Claude Sonnet 4.5 (default)
- **Dataset**: SWE-bench Lite
- **Sample Size**: 10 instances (initial), then 50 or 300

### Hypothesis
Vector-based RAG indexing of the codebase will:
1. Improve code understanding and context retrieval
2. Increase generation rate (better understanding = fewer failures)
3. Improve resolution rate (better fixes from better context)

### Method
1. Install claude-context plugin: https://github.com/zilliztech/claude-context
2. Index each repository before running Claude Code
3. Use default prompt (baseline)

**Command**:
```bash
# Index codebase
claude-context index <repo_path>

# Run with indexed context
python code_swe_agent.py \
  --dataset_name princeton-nlp/SWE-bench_Lite \
  --limit 10 \
  --backend claude
```

### Expected Results
- Generation Rate: 85-90% (better context = fewer confusion failures)
- Resolution Rate: 18-25% (marginal improvement)
- Regression Rate: Similar to baseline (~15%)

### Status
üî¥ **Not Started**

---

## EXP-004: GraphRAG with Test Impact Analysis (Planned)

### Metadata
- **Date**: TBD
- **Configuration**: Custom Claude Code plugin with GraphRAG
- **Model**: Claude Sonnet 4.5 (default)
- **Dataset**: SWE-bench Lite
- **Sample Size**: 10 instances (initial), then 50 or 300

### Hypothesis
Graph-based RAG with explicit test-code relationships will:
1. Enable automatic test impact analysis
2. Catch regressions before completion
3. Achieve lowest regression rate of all configurations

### Method

**Plugin Development**:
1. Create custom Claude Code plugin (see: https://docs.claude.com/en/docs/claude-code/plugins)
2. Build graph structure: nodes = files, edges = dependencies/test relationships
3. Implement test impact analyzer
4. Integrate with Claude Code workflow

**Graph Structure**:
```
Code File ‚Üí Tests That Cover It
Test File ‚Üí Code It Tests
Import ‚Üí Dependency
```

**Workflow**:
1. Agent makes code changes
2. Plugin identifies impacted tests via graph traversal
3. Plugin runs impacted tests in subtask
4. Only complete if tests pass
5. Report any failures to agent for fixing

**Command**:
```bash
python code_swe_agent.py \
  --dataset_name princeton-nlp/SWE-bench_Lite \
  --limit 10 \
  --backend claude \
  --plugin graph-rag-test-impact
```

### Expected Results
- Generation Rate: 75-85% (test failures block some completions)
- Resolution Rate: 25-35% (highest of all configs)
- Regression Rate: <5% (main innovation - tests catch breaks)

### Implementation Plan
1. Week 1: Research Claude Code plugin API
2. Week 2: Build graph builder from AST/imports
3. Week 3: Implement test impact analyzer
4. Week 4: Integrate with Claude Code
5. Week 5: Test and refine

### Status
üî¥ **Not Started**

---

## EXP-004: GraphRAG Test Impact Analysis

### Metadata
- **Date**: November 19, 2025
- **Configuration**: Claude Code + GraphRAG MCP Server + TDD Prompt
- **Model**: Claude Sonnet 4.5
- **Sample Size**: Start with 5-10, scale to 50+
- **Dataset**: SWE-bench Verified
- **Script**: `code_swe_agent_graphrag.py`
- **Prompt**: `prompts/swe_bench_graphrag.txt`

### Hypothesis
GraphRAG-powered test impact analysis will:
1. **Reduce test execution time** by 80-90% (10-50 tests vs 100-500 full suite)
2. **Maintain regression detection** at same level as full test suite
3. **Enable faster feedback loops** for AI agent
4. **Improve resolution rate** via targeted test validation

**Core Innovation**: Intelligent test selection via code-test dependency graph instead of running entire test suite.

### Configuration

**GraphRAG MCP Server Components**:
1. **AST-Based Parser** (`graph_builder.py`)
   - Extracts functions, classes, imports, calls
   - Function-level structural chunking
   - Incremental updates via git diff

2. **Test Linker** (`test_linker.py`)
   - Naming conventions: `test_func` ‚Üí `func`
   - Coverage data: coverage.py integration
   - Static analysis: imports and calls from tests

3. **Impact Analyzer** (`impact_analyzer.py`)
   - Direct testing (score: 1.0)
   - Transitive call dependencies (score: 0.7)
   - Coverage dependencies (score: variable)
   - Import dependencies (score: 0.5)

4. **Neo4j Graph Database** (`graph_db.py`)
   - Nodes: Files, Functions, Classes, Tests
   - Edges: CONTAINS, CALLS, IMPORTS, TESTS, INHERITS, DEPENDS_ON

**Workflow**:
1. Clone repository
2. Build code-test dependency graph (one-time per repo)
3. Execute Claude Code with GraphRAG prompt
4. Detect changed files via git diff
5. Query graph for impacted tests
6. Run only impacted tests (not full suite)
7. Report regressions if any impacted tests fail

### Method

```bash
# Setup
cd /Users/rafaelalonso/Development/Master/Tesis/claudecode_n_codex_swebench
pip install -r requirements_mcp.txt
export NEO4J_EMBEDDED=true

# Quick test (5 instances)
python code_swe_agent_graphrag.py \
  --dataset_name princeton-nlp/SWE-bench_Verified \
  --limit 5 \
  --backend claude

# Full run (50 instances for statistical significance)
python code_swe_agent_graphrag.py \
  --dataset_name princeton-nlp/SWE-bench_Verified \
  --limit 50 \
  --backend claude

# Baseline comparison (no GraphRAG, just TDD)
python code_swe_agent_graphrag.py \
  --dataset_name princeton-nlp/SWE-bench_Verified \
  --limit 5 \
  --backend claude \
  --no-graphrag
```

### Metrics to Track

**Primary Metrics** (same as other experiments):
- Generation Rate: % instances producing non-empty patches
- Resolution Rate: % patches that fix the issue
- Regression Rate: % patches that break existing tests (**PRIMARY METRIC**)

**GraphRAG-Specific Metrics**:
- Graph Build Time: Time to index repository
- Impact Analysis Time: Time to query graph for impacted tests
- Tests Identified: Number of impacted tests found
- Test Efficiency Ratio: Impacted tests / Total tests (target: 0.1-0.2)
- Impact Analysis Accuracy: Did we catch all regressions with subset?

**Time Metrics**:
- Total Time per Instance
- Graph Build Time per Repo (amortized)
- Test Execution Time (impacted only)
- Traditional Test Time (full suite, for comparison)

### Expected Results

**Conservative Estimate**:
- Generation Rate: ~80% (same as baseline)
- Resolution Rate: ~70% (slight improvement from targeted testing)
- Regression Rate: ~10-15% (better than baseline due to TDD + impact analysis)
- Test Efficiency: 10-20 tests instead of 100+ (80-90% time savings)
- Graph Build Time: 30-60s per repository
- Impact Analysis Time: 1-5s per query

**Ambitious Goal**:
- Regression Rate: <5% (ideal state)
- Test Efficiency: >90% time savings
- Impact Accuracy: 100% (no missed regressions)

### Results
*To be filled after running experiment*

**Quick Test (5 instances)**:
- Generation Rate: TBD
- Graph Builds: TBD
- Average Impacted Tests: TBD
- Test Efficiency Ratio: TBD

**Full Run (50 instances)**:
- Generation Rate: TBD
- Resolution Rate: TBD
- Regression Rate: TBD
- Average Test Time Savings: TBD

### Analysis
*To be filled after evaluation*

**Key Questions**:
1. Did GraphRAG identify all tests that would have failed?
2. Were there false negatives (missed impacted tests)?
3. Were there false positives (unnecessary tests run)?
4. What was the actual time savings?
5. Did targeted testing improve or hurt resolution rate?
6. What impact scores (thresholds) worked best?

### Challenges Encountered
*To be documented during execution*

**Technical Challenges**:
- Neo4j setup and configuration
- Graph building performance for large repos
- Coverage.py integration complexity
- Git diff parsing edge cases

**Conceptual Challenges**:
- Determining appropriate impact thresholds
- Handling dynamic dependencies (eval, imports)
- Dealing with test interdependencies
- Validating impact analysis accuracy

### Next Steps
- [ ] Run initial 5-instance test
- [ ] Validate graph building works correctly
- [ ] Verify impact analysis identifies tests
- [ ] Compare against baseline (EXP-001)
- [ ] Scale to 50 instances
- [ ] Evaluate with Docker to get regression rates
- [ ] Compare time savings vs accuracy trade-offs
- [ ] Analyze false negatives/positives
- [ ] Tune impact score thresholds if needed
- [ ] Final 100+ instance evaluation

### Implementation Details

**New Files Created**:
```
mcp_server/
‚îú‚îÄ‚îÄ server.py                    # FastAPI MCP server (406 lines)
‚îú‚îÄ‚îÄ graph_db.py                  # Neo4j database manager (400 lines)
‚îú‚îÄ‚îÄ graph_builder.py             # AST parser & graph builder (450 lines)
‚îú‚îÄ‚îÄ test_linker.py               # Test-to-code linker (370 lines)
‚îú‚îÄ‚îÄ impact_analyzer.py           # Impact analysis logic (330 lines)
‚îú‚îÄ‚îÄ config.py                    # Configuration (130 lines)
‚îî‚îÄ‚îÄ README.md                    # Documentation

utils/
‚îî‚îÄ‚îÄ mcp_graphrag_interface.py   # Client interface (400 lines)

prompts/
‚îî‚îÄ‚îÄ swe_bench_graphrag.txt      # GraphRAG-enhanced TDD prompt (180 lines)

code_swe_agent_graphrag.py      # Evaluation script (600 lines)

requirements_mcp.txt             # Additional dependencies
```

**Total Lines of Code**: ~3,266 lines (new implementation)

### Status
üü° **Implementation Complete - Ready for Testing**

**Completed**:
- ‚úÖ MCP server infrastructure
- ‚úÖ AST-based code parsing
- ‚úÖ Neo4j graph database
- ‚úÖ Test linking strategies
- ‚úÖ Impact analysis algorithms
- ‚úÖ Client interface
- ‚úÖ Evaluation script
- ‚úÖ Documentation

**Next**:
- ‚è≥ Initial 5-instance test run
- ‚è≥ Validation and debugging
- ‚è≥ Full experimental evaluation

---

## Comparison Matrix

### Experiment Comparison

| Experiment | Dataset | Generation Rate | Resolution Rate | Regression Rate | Avg Time | Status |
|------------|---------|----------------|-----------------|-----------------|----------|--------|
| EXP-001: Baseline (Sonnet) | Lite | 80% (8/10) | TBD | TBD | 4.9 min | ‚è≥ Eval Pending |
| EXP-001B: Haiku 4.5 | Lite | 100% (1/1) | TBD | TBD | 2.6 min | ‚úÖ Complete |
| EXP-001B: Sonnet 4.5 | Lite | 100% (1/1) | TBD | TBD | 4.2 min | ‚úÖ Complete |
| EXP-001C: Haiku 4.5 | Verified | 100% (1/1) | TBD | TBD | 2.1 min | ‚úÖ Complete |
| EXP-002: TDD Prompt | TBD | TBD | TBD | TBD | TBD | üî¥ Not Started |
| EXP-003: Vector RAG | TBD | TBD | TBD | TBD | TBD | üî¥ Not Started |
| EXP-004: GraphRAG | Verified | TBD | TBD | TBD | TBD | üü° Ready to Test |

### Model Comparison (EXP-001B + EXP-001C)

**EXP-001B: Haiku vs Sonnet on SWE-bench Lite**
| Metric | Haiku 4.5 | Sonnet 4.5 | Winner |
|--------|-----------|------------|--------|
| **Speed** | 2:35 (155s) | 4:09 (250s) | Haiku (61% faster) |
| **Patch Size** | 506 chars | 12,644 chars | Haiku (minimal) |
| **Cost** | ~$0.005 | ~$0.05-0.10 | Haiku (10-20x cheaper) |
| **Comprehensiveness** | Minimal | Extensive | Sonnet |
| **Test Coverage** | None | Comprehensive | Sonnet |
| **Core Fix** | ‚úÖ Correct | ‚úÖ Correct | Tie |
| **Files Modified** | 1 | 6 | Haiku (focused) |

**Haiku Variability Analysis (Same Instance)**
| Run | Dataset | Patch Size | Test Coverage | Runtime | Consistency |
|-----|---------|------------|---------------|---------|-------------|
| EXP-001B | Lite | 506 chars | None | 2:35 | Minimal mode |
| EXP-001C | Verified | 2,037 chars | Comprehensive | 2:04 | Comprehensive mode |
| **Variance** | - | **4x difference** | Major variance | 20% faster | Non-deterministic |

**Key Finding**: Haiku produces significantly different patches for the same instance, varying from minimal (506) to comprehensive (2,037) approaches. Sonnet is more consistent.

**Target**: Minimize regression rate while maintaining >70% generation rate

---

## Notes & Observations

### General Patterns
- (To be filled as experiments progress)

### Unexpected Findings
- (To be filled as experiments progress)

### Open Questions
1. How to properly measure regression rate?
   - Option A: Run full test suite and count new failures
   - Option B: Use SWE-bench's built-in evaluation
   - Option C: Manual code review

2. What sample size is statistically significant?
   - 10 instances: Quick feedback, high variance
   - 50 instances: ~4 hours, moderate confidence
   - 300 instances: ~25 hours, high confidence

3. Should we test on same 10/50/300 instances across all experiments?
   - Pro: Direct comparison, controlled
   - Con: Might overfit to specific issues

---

## Appendix

### Hardware Specifications
- **Machine**: MacBook (Apple Silicon)
- **RAM**: 16GB+ recommended for Docker
- **Disk**: 50GB+ free for Docker images
- **Docker Memory Allocation**: 8GB+

### Software Versions
- **Claude Code**: v2.0.28
- **Python**: 3.13.9
- **Docker**: Latest
- **SWE-bench**: 4.1.0

### Key References
- [SWE-bench Paper](https://arxiv.org/abs/2310.06770)
- [Anthropic's SWE-bench Results](https://www.anthropic.com/engineering/swe-bench-sonnet)
- [Claude Code Documentation](https://docs.claude.com/en/docs/claude-code)
- [GraphRAG Paper](https://arxiv.org/abs/2404.16130)

---

**Last Updated**: October 28, 2025 23:20
**Next Update**: After larger SWE-bench Verified baseline (10-50 instances) or EXP-002 TDD prompts


---

## EXP-005: Full Three-Way Comparison (50 Instances Each)

### Metadata
- **Date**: 2025-11-20 04:01
- **Dataset**: SWE-bench_Verified
- **Sample Size**: 50 instances per experiment
- **Experiments**: Baseline, TDD, GraphRAG

### Executive Summary

**Winner**: üèÜ **Baseline**

**Key Findings:**
- Baseline achieved the highest generation rate (46.0%), 46.0% better than GraphRAG
- Baseline produced the largest patches on average (9292 chars), suggesting more comprehensive fixes
- Baseline had the fewest errors (27), indicating better stability
- GraphRAG identified an average of 0.0 impacted tests per instance, with graph building taking 0.0s on average

### Detailed Metrics Comparison

| Metric | Baseline | TDD | GraphRAG |
|--------|----------|-----|----------|
| **Generation Rate** | 46.0% | 0.0% | 0.0% |
| **Avg Patch Size** | 9,292 chars | 0 chars | 0 chars |
| **Median Patch Size** | 6,453 chars | 0 chars | 0 chars |
| **Errors** | 27 | 50 | 50 |

### GraphRAG-Specific Metrics

- **Total Graphs Built**: 0
- **Avg Graph Build Time**: 0.0s
- **Avg Impacted Tests Found**: 0.0 tests
- **Avg Impact Analysis Time**: 0.00s
- **Test Range**: 0 - 0 tests

### Error Analysis

**Baseline Errors:**
- Execution Failed: 21
- Repository Setup: 6

**TDD Errors:**
- Execution Failed: 50

**GraphRAG Errors:**
- Execution Failed: 39
- Repository Setup: 11

### Recommendations

1. Use Baseline for production SWE-bench evaluation based on overall performance
2. Investigate why TDD has low generation rate - may need prompt refinement
3. Run Docker evaluation to measure actual resolution and regression rates for GraphRAG

### Next Steps

- [ ] Run Docker evaluation on all three prediction sets
- [ ] Calculate resolution rates from evaluation results
- [ ] Measure regression rates for each approach
- [ ] Compare actual test execution times
- [ ] Analyze specific instances where approaches differed

### Prediction Files

- **Baseline**: `predictions_20251120_010951.jsonl`
- **TDD**: `predictions_20251120_032821.jsonl`
- **GraphRAG**: `predictions_graphrag_20251120_034523.jsonl`
